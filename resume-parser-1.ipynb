{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESUME PARSER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:43.867787Z",
     "iopub.status.busy": "2022-01-08T15:28:43.866386Z",
     "iopub.status.idle": "2022-01-08T15:28:52.719225Z",
     "shell.execute_reply": "2022-01-08T15:28:52.717825Z",
     "shell.execute_reply.started": "2022-01-08T15:28:43.867554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading tika-1.24.tar.gz (28 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sub13\\anaconda3\\lib\\site-packages (from tika) (61.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sub13\\anaconda3\\lib\\site-packages (from tika) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sub13\\anaconda3\\lib\\site-packages (from requests->tika) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sub13\\anaconda3\\lib\\site-packages (from requests->tika) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sub13\\anaconda3\\lib\\site-packages (from requests->tika) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sub13\\anaconda3\\lib\\site-packages (from requests->tika) (1.26.9)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32891 sha256=4c881ae8f21a5d744659be8d8c079bdabbc111213a426f50c30a633ebed8e8e0\n",
      "  Stored in directory: c:\\users\\sub13\\appdata\\local\\pip\\cache\\wheels\\ec\\76\\38\\0e4b92d8a3a89cbfff5be03a40c02d15b2072b1b08ebf28d6a\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-1.24\n"
     ]
    }
   ],
   "source": [
    "!pip install tika"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:52.723556Z",
     "iopub.status.busy": "2022-01-08T15:28:52.722507Z",
     "iopub.status.idle": "2022-01-08T15:28:52.817693Z",
     "shell.execute_reply": "2022-01-08T15:28:52.816641Z",
     "shell.execute_reply.started": "2022-01-08T15:28:52.723487Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 23:44:02,358 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to C:\\Users\\sub13\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2022-10-05 23:44:21,149 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to C:\\Users\\sub13\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2022-10-05 23:44:22,413 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Subodh Bharti\n",
      "Data Sientist intern\n",
      "\n",
      "+91 8778945019 �\n",
      "\n",
      "subodhbhartismiley@gmail.com !\n",
      "\n",
      "Bihar (\n",
      "\n",
      "¥ https://github.com/subodhbhartismiley\n",
      "è https://www.linkedin.com/in/subodh-bharti-\n",
      "\n",
      "368433104/\n",
      "\n",
      "About Me\n",
      "\n",
      "To obtain a position through which I\n",
      "\n",
      "would be able to utilize my SKA for\n",
      "\n",
      "provided role and I am looking forward\n",
      "\n",
      "for innovative challenges in development\n",
      "\n",
      "technologies.\n",
      "\n",
      "Education\n",
      "Pondicherry University Pondicherry\n",
      "\n",
      "2014 - 2022\n",
      "\n",
      "M.Sc-Statistics\n",
      "\n",
      "Skills\n",
      "\n",
      "data science\n",
      "\n",
      "Data Analysis\n",
      "NLP\n",
      "\n",
      "ML\n",
      "\n",
      "Machine Learning\n",
      "\n",
      "Statistical Analysis\n",
      "EDA\n",
      "\n",
      "Power BI Desktop\n",
      "SQL\n",
      "\n",
      "MySQL\n",
      "\n",
      "Github\n",
      "\n",
      "Sentiment Analysis\n",
      "\n",
      "Artificial Intelligence\n",
      "\n",
      "Big Data\n",
      "\n",
      "R progrmming\n",
      "\n",
      "Python\n",
      "\n",
      "pandas\n",
      "\n",
      "Matplotlib\n",
      "\n",
      "Translator\n",
      "\n",
      "Experience\n",
      "\n",
      "Pondicherry University, Department of statistics-Project work, ma-\n",
      "\n",
      "jor\n",
      "\n",
      "2020 - 2020\n",
      "\n",
      "PROJECT Title “Sentiment Analysis of tags (covid19 a Pandemic of\n",
      "\n",
      "the Decade) From Twitter Data” six month as a part of Master Degree\n",
      "\n",
      "(M.Sc.).\n",
      "\n",
      "Used Python programming language NLP ML Tweepy Framework\n",
      "\n",
      "iNeuronai-Data Science Intern\n",
      "\n",
      "2022 - Present\n",
      "\n",
      "Doing Internship in ineuron under the guidance of Mr. Sudhanshu.\n",
      "\n",
      "And learning data analysis skills big data EDA etc.\n",
      "\n",
      "Projects\n",
      "\n",
      "Sentiment Analysis of covid-19 of Twitter Data\n",
      "\n",
      "In this project I used tweeter api for accessing tweeter data used NLP\n",
      "\n",
      "ML and found For million of tweet analysed we got Insight as positive\n",
      "\n",
      "tweets percentage are far higher as People sentiments more positive\n",
      "\n",
      "than negative towards the covid19.\n",
      "\n",
      "Achievements\n",
      "Rank 1 and 5 star on with score of 2305 points, for basic and Interme-\n",
      "\n",
      "diate Python programming and 5 star for MySQL skill on HackerRank\n",
      "\n",
      "Website. Have hands on EDA projects, data analysis, sales analysis.\n",
      "\n",
      "1\n",
      "\n",
      "https://github.com/https://github.com/subodhbhartismiley\n",
      "https://linkedin.com/https://www.linkedin.com/in/subodh-bharti-368433104/\n",
      "https://linkedin.com/https://www.linkedin.com/in/subodh-bharti-368433104/\n",
      "https://github.com/subodhbhartismiley/Sentiment_Analysis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tika import parser\n",
    "# \"C:\\Users\\sub13\\Downloads\\Subodh Bharti's Resume.pdf\"\n",
    "file = 'C:/Users/sub13/Downloads/Subodh Bharti Resume.pdf'\n",
    "file_data = parser.from_file(file)\n",
    "text = file_data['content']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dictionary to store parsed content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:52.820525Z",
     "iopub.status.busy": "2022-01-08T15:28:52.819043Z",
     "iopub.status.idle": "2022-01-08T15:28:52.826295Z",
     "shell.execute_reply": "2022-01-08T15:28:52.825109Z",
     "shell.execute_reply.started": "2022-01-08T15:28:52.820451Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed_content = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting E-Mail from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:52.831164Z",
     "iopub.status.busy": "2022-01-08T15:28:52.830053Z",
     "iopub.status.idle": "2022-01-08T15:28:52.844101Z",
     "shell.execute_reply": "2022-01-08T15:28:52.843057Z",
     "shell.execute_reply.started": "2022-01-08T15:28:52.831105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subodhbhartismiley@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "#E-MAIL\n",
    "import re\n",
    "def get_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "\n",
    "email = get_email_addresses(text)\n",
    "print(email)\n",
    "parsed_content['E-mail'] = email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Phone Number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:52.848212Z",
     "iopub.status.busy": "2022-01-08T15:28:52.847300Z",
     "iopub.status.idle": "2022-01-08T15:28:52.861423Z",
     "shell.execute_reply": "2022-01-08T15:28:52.860431Z",
     "shell.execute_reply.started": "2022-01-08T15:28:52.848156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8778945019', '3684331', '3684331', '3684331']\n"
     ]
    }
   ],
   "source": [
    "#PHONE NUMBER\n",
    "import re\n",
    "def get_phone_numbers(string):\n",
    "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n",
    "\n",
    "phone_number= get_phone_numbers(text)\n",
    "if len(phone_number) <= 10:\n",
    "    print(phone_number)\n",
    "    parsed_content['Phone number'] = phone_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:52.865088Z",
     "iopub.status.busy": "2022-01-08T15:28:52.864292Z",
     "iopub.status.idle": "2022-01-08T15:28:58.692795Z",
     "shell.execute_reply": "2022-01-08T15:28:58.691793Z",
     "shell.execute_reply.started": "2022-01-08T15:28:52.864973Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m      4\u001b[0m matcher \u001b[38;5;241m=\u001b[39m Matcher(nlp\u001b[38;5;241m.\u001b[39mvocab)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py:436\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(text):\n",
    "   nlp_text = nlp(text)\n",
    "  \n",
    "   # First name and Last name are always Proper Nouns\n",
    "   pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "  \n",
    "   matcher.add('NAME', [pattern], on_match = None)\n",
    "  \n",
    "   matches = matcher(nlp_text)\n",
    "  \n",
    "   for match_id, start, end in matches:\n",
    "       span = nlp_text[start:end]\n",
    "       return span.text\n",
    "\n",
    "name = extract_name(text)\n",
    "print(name)\n",
    "parsed_content['Name'] =  name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Keywords to identify 'Headings' in the Resume text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.695642Z",
     "iopub.status.busy": "2022-01-08T15:28:58.694557Z",
     "iopub.status.idle": "2022-01-08T15:28:58.700786Z",
     "shell.execute_reply": "2022-01-08T15:28:58.700202Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.695572Z"
    }
   },
   "outputs": [],
   "source": [
    "Keywords = [\"education\",\n",
    "            \"summary\",\n",
    "            \"accomplishments\",\n",
    "            \"executive profile\",\n",
    "            \"professional profile\",\n",
    "            \"personal profile\",\n",
    "            \"work background\",\n",
    "            \"academic profile\",\n",
    "            \"other activities\",\n",
    "            \"qualifications\",\n",
    "            \"experience\",\n",
    "            \"interests\",\n",
    "            \"skills\",\n",
    "            \"achievements\",\n",
    "            \"publications\",\n",
    "            \"publication\",\n",
    "            \"certifications\",\n",
    "            \"workshops\",\n",
    "            \"projects\",\n",
    "            \"internships\",\n",
    "            \"trainings\",\n",
    "            \"hobbies\",\n",
    "            \"overview\",\n",
    "            \"objective\",\n",
    "            \"position of responsibility\",\n",
    "            \"jobs\"\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the resume text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.703027Z",
     "iopub.status.busy": "2022-01-08T15:28:58.702058Z",
     "iopub.status.idle": "2022-01-08T15:28:58.725847Z",
     "shell.execute_reply": "2022-01-08T15:28:58.724919Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.702991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                          subodh bharti data sientist intern  +91 8778945019 �  subodhbhartismiley@gmail.com !  bihar (  ¥ https://github.com/subodhbhartismiley è https://www.linkedin.com/in/subodh-bharti-  368433104/  about me  to obtain a position through which i  would be able to utilize my ska for  provided role and i am looking forward  for innovative challenges in development  technologies.  education pondicherry university pondicherry  2014 - 2022  m.sc-statistics  skills  data science  data analysis nlp  ml  machine learning  statistical analysis eda  power bi desktop sql  mysql  github  sentiment analysis  artificial intelligence  big data  r progrmming  python  pandas  matplotlib  translator  experience  pondicherry university, department of statistics-project work, ma-  jor  2020 - 2020  project title “sentiment analysis of tags (covid19 a pandemic of  the decade) from twitter data” six month as a part of master degree  (m.sc.).  used python programming language nlp ml tweepy framework  ineuronai-data science intern  2022 - present  doing internship in ineuron under the guidance of mr. sudhanshu.  and learning data analysis skills big data eda etc.  projects  sentiment analysis of covid-19 of twitter data  in this project i used tweeter api for accessing tweeter data used nlp  ml and found for million of tweet analysed we got insight as positive  tweets percentage are far higher as people sentiments more positive  than negative towards the covid19.  achievements rank 1 and 5 star on with score of 2305 points, for basic and interme-  diate python programming and 5 star for mysql skill on hackerrank  website. have hands on eda projects, data analysis, sales analysis.  1  https://github.com/https://github.com/subodhbhartismiley https://linkedin.com/https://www.linkedin.com/in/subodh-bharti-368433104/ https://linkedin.com/https://www.linkedin.com/in/subodh-bharti-368433104/ https://github.com/subodhbhartismiley/sentiment_analysis  \n"
     ]
    }
   ],
   "source": [
    "text = text.replace(\"\\n\",\" \")\n",
    "text = text.replace(\"[^a-zA-Z0-9]\", \" \");  \n",
    "re.sub('\\W+','', text)\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the headings and corresponding indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.728319Z",
     "iopub.status.busy": "2022-01-08T15:28:58.727395Z",
     "iopub.status.idle": "2022-01-08T15:28:58.739898Z",
     "shell.execute_reply": "2022-01-08T15:28:58.738820Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.728266Z"
    }
   },
   "outputs": [],
   "source": [
    "content = {}\n",
    "indices = []\n",
    "keys = []\n",
    "for key in Keywords:\n",
    "    try:\n",
    "        content[key] = text[text.index(key) + len(key):]\n",
    "        indices.append(text.index(key))\n",
    "        keys.append(key)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.742989Z",
     "iopub.status.busy": "2022-01-08T15:28:58.742700Z",
     "iopub.status.idle": "2022-01-08T15:28:58.756719Z",
     "shell.execute_reply": "2022-01-08T15:28:58.755546Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.742954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['education', 'skills', 'experience', 'projects', 'achievements']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting the indices\n",
    "zipped_lists = zip(indices, keys)\n",
    "sorted_pairs = sorted(zipped_lists)\n",
    "sorted_pairs\n",
    "\n",
    "tuples = zip(*sorted_pairs)\n",
    "indices, keys = [ list(tuple) for tuple in  tuples]\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.758818Z",
     "iopub.status.busy": "2022-01-08T15:28:58.758575Z",
     "iopub.status.idle": "2022-01-08T15:28:58.768702Z",
     "shell.execute_reply": "2022-01-08T15:28:58.767839Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.758788Z"
    }
   },
   "outputs": [],
   "source": [
    "#Keeping the required content and removing the redundant part\n",
    "content = []\n",
    "for idx in range(len(indices)):\n",
    "    if idx != len(indices)-1:\n",
    "        content.append(text[indices[idx]: indices[idx+1]])\n",
    "    else:\n",
    "        content.append(text[indices[idx]: ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the parsed content in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.770671Z",
     "iopub.status.busy": "2022-01-08T15:28:58.769826Z",
     "iopub.status.idle": "2022-01-08T15:28:58.781014Z",
     "shell.execute_reply": "2022-01-08T15:28:58.780318Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.770635Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(indices)):\n",
    "    parsed_content[keys[i]] = content[i]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.782909Z",
     "iopub.status.busy": "2022-01-08T15:28:58.782237Z",
     "iopub.status.idle": "2022-01-08T15:28:58.796319Z",
     "shell.execute_reply": "2022-01-08T15:28:58.795038Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.782877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E-mail': ['subodhbhartismiley@gmail.com'],\n",
       " 'Phone number': ['8778945019', '3684331', '3684331', '3684331'],\n",
       " 'education': 'education pondicherry university pondicherry  2014 - 2022  m.sc-statistics  ',\n",
       " 'skills': 'skills  data science  data analysis nlp  ml  machine learning  statistical analysis eda  power bi desktop sql  mysql  github  sentiment analysis  artificial intelligence  big data  r progrmming  python  pandas  matplotlib  translator  ',\n",
       " 'experience': 'experience  pondicherry university, department of statistics-project work, ma-  jor  2020 - 2020  project title “sentiment analysis of tags (covid19 a pandemic of  the decade) from twitter data” six month as a part of master degree  (m.sc.).  used python programming language nlp ml tweepy framework  ineuronai-data science intern  2022 - present  doing internship in ineuron under the guidance of mr. sudhanshu.  and learning data analysis skills big data eda etc.  ',\n",
       " 'projects': 'projects  sentiment analysis of covid-19 of twitter data  in this project i used tweeter api for accessing tweeter data used nlp  ml and found for million of tweet analysed we got insight as positive  tweets percentage are far higher as people sentiments more positive  than negative towards the covid19.  ',\n",
       " 'achievements': 'achievements rank 1 and 5 star on with score of 2305 points, for basic and interme-  diate python programming and 5 star for mysql skill on hackerrank  website. have hands on eda projects, data analysis, sales analysis.  1  https://github.com/https://github.com/subodhbhartismiley https://linkedin.com/https://www.linkedin.com/in/subodh-bharti-368433104/ https://linkedin.com/https://www.linkedin.com/in/subodh-bharti-368433104/ https://github.com/subodhbhartismiley/sentiment_analysis  '}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the parsed content\n",
    "parsed_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping the dictionary into json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.798404Z",
     "iopub.status.busy": "2022-01-08T15:28:58.797926Z",
     "iopub.status.idle": "2022-01-08T15:28:58.807320Z",
     "shell.execute_reply": "2022-01-08T15:28:58.806283Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.798336Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Parsed_Resume.json\", \"w\") as outfile:\n",
    "    json.dump(parsed_content, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the contents of json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-08T15:28:58.808922Z",
     "iopub.status.busy": "2022-01-08T15:28:58.808469Z",
     "iopub.status.idle": "2022-01-08T15:28:58.820937Z",
     "shell.execute_reply": "2022-01-08T15:28:58.819814Z",
     "shell.execute_reply.started": "2022-01-08T15:28:58.808885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"E-mail\": [\n",
      "        \"subodhbhartismiley@gmail.com\"\n",
      "    ],\n",
      "    \"Phone number\": [\n",
      "        \"8778945019\",\n",
      "        \"3684331\",\n",
      "        \"3684331\",\n",
      "        \"3684331\"\n",
      "    ],\n",
      "    \"education\": \"education pondicherry university pondicherry  2014 - 2022  m.sc-statistics  \",\n",
      "    \"skills\": \"skills  data science  data analysis nlp  ml  machine learning  statistical analysis eda  power bi desktop sql  mysql  github  sentiment analysis  artificial intelligence  big data  r progrmming  python  pandas  matplotlib  translator  \",\n",
      "    \"experience\": \"experience  pondicherry university, department of statistics-project work, ma-  jor  2020 - 2020  project title \\u201csentiment analysis of tags (covid19 a pandemic of  the decade) from twitter data\\u201d six month as a part of master degree  (m.sc.).  used python programming language nlp ml tweepy framework  ineuronai-data science intern  2022 - present  doing internship in ineuron under the guidance of mr. sudhanshu.  and learning data analysis skills big data eda etc.  \",\n",
      "    \"projects\": \"projects  sentiment analysis of covid-19 of twitter data  in this project i used tweeter api for accessing tweeter data used nlp  ml and found for million of tweet analysed we got insight as positive  tweets percentage are far higher as people sentiments more positive  than negative towards the covid19.  \",\n",
      "    \"achievements\": \"achievements rank 1 and 5 star on with score of 2305 points, for basic and interme-  diate python programming and 5 star for mysql skill on hackerrank  website. have hands on eda projects, data analysis, sales analysis.  1  https://github.com/https://github.com/subodhbhartismiley https://linkedin.com/https://www.linkedin.com/in/subodh-bharti-368433104/ https://linkedin.com/https://www.linkedin.com/in/subodh-bharti-368433104/ https://github.com/subodhbhartismiley/sentiment_analysis  \"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "a_file = open(\"Parsed_Resume.json\", \"r\")\n",
    "a_json = json.load(a_file)\n",
    "pretty_json = json.dumps(a_json, indent=4)\n",
    "a_file.close()\n",
    "print(pretty_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-i 4\n",
      "[1, 2, 3, 4]\n",
      "n-i 2\n",
      "[2, 3, 4]\n",
      "n-i 0\n",
      "[3, 4]\n",
      "n-i -2\n",
      "[]\n",
      "n-i -4\n",
      "[5, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "arr=[1,2,3,4,5]\n",
    "n=4\n",
    "for i in range(5):\n",
    "    print(\"n-i\", n-i)\n",
    "    if n-i<0:\n",
    "        print(arr[i:n-i:-1])\n",
    "    else:\n",
    "        print(arr[i:n+i])\n",
    "    n=n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
